{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "# Exploring the Influence of City and Temperature on US Immigration\n",
    "### Data Engineering Capstone Project\n",
    "\n",
    "#### Project Summary\n",
    "This capstone project creates an analytics database to support end user queries exploring the influences of destination city and temperature on US immigration. \n",
    "\n",
    "Potential user queries include:\n",
    "* Visitors by age, gender, visa category, and country of origin.\n",
    "* Relationship between U.S state destination and country of origin.\n",
    "\n",
    "Future user queries might include:\n",
    "* Relationship between U.S state destination and climate in country of origin.\n",
    "* Relationship between U.S. state demographics and country of origin.\n",
    "* Relationship between U.S. state demographics and climate in country of origin.\n",
    "\n",
    "The project follows the following steps:\n",
    "* Step 1: Scope the Project and Gather Data\n",
    "* Step 2: Explore and Assess the Data\n",
    "* Step 3: Define the Data Model\n",
    "* Step 4: Run ETL to Model the Data\n",
    "* Step 5: Complete Project Write Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Import required libraries and modules\n",
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 1: Scope the Project and Gather Data\n",
    "\n",
    "#### Scope \n",
    "The project involves building an ETL pipeline that extracts the data from the four datasets, processes them using Spark, and loads the data into as a set of dimensional tables based on the star schema. \n",
    "\n",
    "The data originates from four datasets. The main dataset includes data on U.S. immigration, while supplementary datasets include data on airport codes, U.S. city demographics, and temperature data.\n",
    "\n",
    "#### Data Description \n",
    "\n",
    "U.S. Immigration Data:\n",
    "- **Data Source**: The U.S. immigration data originates from the [National Travel & Tourism Office](https://travel.trade.gov/research/reports/i94/historical/2016.html). \n",
    "- **Description**: Each report contains international visitor arrival statistics by world regions and select countries (including top 20), type of visa, mode of transportation, age groups, states visited (first intended address only), and the top ports of entry (for select countries). Data sources include: Overseas DHS/CBP I-94 Program data; Canadian visitation data (Stats Canada) and Mexican visitation data (Banco de Mexico). \n",
    "- **Information Included**: Udacity has provided 2016 data in the folder ```../../data/18-83510-I94-Data-2016/``` in SAS7BDAT binary storage format. Udacity has also provided April 2016 data in the folder ```./sas_data``` and sample data in the file ```./immigration_data_sample.csv```. The April 2016 files have 28 columns and 3,096,313 rows of data in Snappy Parquet format, while the sample data file has 28 columns and 1,000 rows of data in CSV format. Data is also available for purchase from the National Travel & Tourism Office website.\n",
    "\n",
    "U.S. City Demographic Data: \n",
    "- **Data Source**: The U.S. city demographic data originates from [OpenSoft](https://public.opendatasoft.com/explore/dataset/us-cities-demographics/export/).\n",
    "- **Description**: This dataset contains information about the demographics of all US cities and census-designated places with a population greater or equal to 65,000. This data comes from the US Census Bureau's 2015 American Community Survey.\n",
    "- **Information Included**: Udacity has provided sample data in the following path: ```./us-cities-demographics.csv```. The file has 12 columns and 2,891 rows of data. Data is also available from OpenSoft in CSV, JSON, or Excel formats. \n",
    "\n",
    "Global Temperature Data:\n",
    "- **Data Source**: The global temperature data originates from Kaggle's [Climate Change: Earth Surface Temperature Data](https://www.kaggle.com/berkeleyearth/climate-change-earth-surface-temperature-data) dataset.\n",
    "- **Description**: Kaggle has repackaged the data from a newer compilation put together by the Berkeley Earth, which is affiliated with Lawrence Berkeley National Laboratory. The Berkeley Earth Surface Temperature Study combines 1.6 billion temperature reports from 16 pre-existing archives. The data starts in 1750 for average land temperature and 1850 for max and min land temperatures and global ocean and land temperatures.\n",
    "- **Information Included**: Udacity has provided data in a folder with the following path: ```../../data2/```. There's just one file in that folder, called ```GlobalLandTemperaturesByCity.csv```. The file has 7 columns and 8,599,212 rows of data in CSV format. Data is also available from the Kaggle dataset, which inlcudes 4 additional files:\n",
    "  - GlobalLandTemperaturesByCountry.csv\n",
    "  - GlobalLandTemperaturesByMajorCity.csv\n",
    "  - GlobalLandTemperaturesByState.csv\n",
    "  - GlobalLandTemperaturesByState.csv\n",
    "\n",
    "World Airport Code Table: \n",
    "- **Data Source**: The world airport code table originates from [DataHub](https://datahub.io/core/airport-codes#data).\n",
    "- **Description**: The airport codes may refer to either IATA airport code, a three-letter code which is used in passenger reservation, ticketing and baggage-handling systems, or the ICAO airport code which is a four letter code used by ATC systems and for airports that do not have an IATA airport code (from wikipedia). Airport codes from around the world. Downloaded from public domain source http://ourairports.com/data/ who compiled this data from multiple different sources. This data is updated nightly.\n",
    "- **Information Included**: Udacity has provided the data in the following path: ```./airport-codes_csv.csv```. The file has 12 columns and 55,075 rows of data in CSV format. Data is also available from DataHub. \n",
    "\n",
    "\n",
    "#### Data Gathering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Read in the data here, including sample immigration data\n",
    "df_immigration_sample = pd.read_csv('./immigration_data_sample.csv')\n",
    "df_demographics = pd.read_csv('./us-cities-demographics.csv', sep=';')\n",
    "df_temperatures = pd.read_csv('../../data2/GlobalLandTemperaturesByCity.csv')\n",
    "df_airport_codes = pd.read_csv('./airport-codes_csv.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 29)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>cicid</th>\n",
       "      <th>i94yr</th>\n",
       "      <th>i94mon</th>\n",
       "      <th>i94cit</th>\n",
       "      <th>i94res</th>\n",
       "      <th>i94port</th>\n",
       "      <th>arrdate</th>\n",
       "      <th>i94mode</th>\n",
       "      <th>i94addr</th>\n",
       "      <th>...</th>\n",
       "      <th>entdepu</th>\n",
       "      <th>matflag</th>\n",
       "      <th>biryear</th>\n",
       "      <th>dtaddto</th>\n",
       "      <th>gender</th>\n",
       "      <th>insnum</th>\n",
       "      <th>airline</th>\n",
       "      <th>admnum</th>\n",
       "      <th>fltno</th>\n",
       "      <th>visatype</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2027561</td>\n",
       "      <td>4084316.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>209.0</td>\n",
       "      <td>209.0</td>\n",
       "      <td>HHW</td>\n",
       "      <td>20566.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>HI</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M</td>\n",
       "      <td>1955.0</td>\n",
       "      <td>07202016</td>\n",
       "      <td>F</td>\n",
       "      <td>NaN</td>\n",
       "      <td>JL</td>\n",
       "      <td>5.658267e+10</td>\n",
       "      <td>00782</td>\n",
       "      <td>WT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2171295</td>\n",
       "      <td>4422636.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>582.0</td>\n",
       "      <td>582.0</td>\n",
       "      <td>MCA</td>\n",
       "      <td>20567.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>TX</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M</td>\n",
       "      <td>1990.0</td>\n",
       "      <td>10222016</td>\n",
       "      <td>M</td>\n",
       "      <td>NaN</td>\n",
       "      <td>*GA</td>\n",
       "      <td>9.436200e+10</td>\n",
       "      <td>XBLNG</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>589494</td>\n",
       "      <td>1195600.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>148.0</td>\n",
       "      <td>112.0</td>\n",
       "      <td>OGG</td>\n",
       "      <td>20551.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>FL</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M</td>\n",
       "      <td>1940.0</td>\n",
       "      <td>07052016</td>\n",
       "      <td>M</td>\n",
       "      <td>NaN</td>\n",
       "      <td>LH</td>\n",
       "      <td>5.578047e+10</td>\n",
       "      <td>00464</td>\n",
       "      <td>WT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2631158</td>\n",
       "      <td>5291768.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>297.0</td>\n",
       "      <td>297.0</td>\n",
       "      <td>LOS</td>\n",
       "      <td>20572.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>CA</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M</td>\n",
       "      <td>1991.0</td>\n",
       "      <td>10272016</td>\n",
       "      <td>M</td>\n",
       "      <td>NaN</td>\n",
       "      <td>QR</td>\n",
       "      <td>9.478970e+10</td>\n",
       "      <td>00739</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3032257</td>\n",
       "      <td>985523.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>111.0</td>\n",
       "      <td>111.0</td>\n",
       "      <td>CHM</td>\n",
       "      <td>20550.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NY</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M</td>\n",
       "      <td>1997.0</td>\n",
       "      <td>07042016</td>\n",
       "      <td>F</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.232257e+10</td>\n",
       "      <td>LAND</td>\n",
       "      <td>WT</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0      cicid   i94yr  i94mon  i94cit  i94res i94port  arrdate  \\\n",
       "0     2027561  4084316.0  2016.0     4.0   209.0   209.0     HHW  20566.0   \n",
       "1     2171295  4422636.0  2016.0     4.0   582.0   582.0     MCA  20567.0   \n",
       "2      589494  1195600.0  2016.0     4.0   148.0   112.0     OGG  20551.0   \n",
       "3     2631158  5291768.0  2016.0     4.0   297.0   297.0     LOS  20572.0   \n",
       "4     3032257   985523.0  2016.0     4.0   111.0   111.0     CHM  20550.0   \n",
       "\n",
       "   i94mode i94addr    ...     entdepu  matflag  biryear   dtaddto  gender  \\\n",
       "0      1.0      HI    ...         NaN        M   1955.0  07202016       F   \n",
       "1      1.0      TX    ...         NaN        M   1990.0  10222016       M   \n",
       "2      1.0      FL    ...         NaN        M   1940.0  07052016       M   \n",
       "3      1.0      CA    ...         NaN        M   1991.0  10272016       M   \n",
       "4      3.0      NY    ...         NaN        M   1997.0  07042016       F   \n",
       "\n",
       "  insnum airline        admnum  fltno  visatype  \n",
       "0    NaN      JL  5.658267e+10  00782        WT  \n",
       "1    NaN     *GA  9.436200e+10  XBLNG        B2  \n",
       "2    NaN      LH  5.578047e+10  00464        WT  \n",
       "3    NaN      QR  9.478970e+10  00739        B2  \n",
       "4    NaN     NaN  4.232257e+10   LAND        WT  \n",
       "\n",
       "[5 rows x 29 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display first 5 rows of each dataset\n",
    "print(df_immigration_sample.shape)\n",
    "df_immigration_sample.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2891, 12)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>City</th>\n",
       "      <th>State</th>\n",
       "      <th>Median Age</th>\n",
       "      <th>Male Population</th>\n",
       "      <th>Female Population</th>\n",
       "      <th>Total Population</th>\n",
       "      <th>Number of Veterans</th>\n",
       "      <th>Foreign-born</th>\n",
       "      <th>Average Household Size</th>\n",
       "      <th>State Code</th>\n",
       "      <th>Race</th>\n",
       "      <th>Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Silver Spring</td>\n",
       "      <td>Maryland</td>\n",
       "      <td>33.8</td>\n",
       "      <td>40601.0</td>\n",
       "      <td>41862.0</td>\n",
       "      <td>82463</td>\n",
       "      <td>1562.0</td>\n",
       "      <td>30908.0</td>\n",
       "      <td>2.60</td>\n",
       "      <td>MD</td>\n",
       "      <td>Hispanic or Latino</td>\n",
       "      <td>25924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Quincy</td>\n",
       "      <td>Massachusetts</td>\n",
       "      <td>41.0</td>\n",
       "      <td>44129.0</td>\n",
       "      <td>49500.0</td>\n",
       "      <td>93629</td>\n",
       "      <td>4147.0</td>\n",
       "      <td>32935.0</td>\n",
       "      <td>2.39</td>\n",
       "      <td>MA</td>\n",
       "      <td>White</td>\n",
       "      <td>58723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hoover</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>38.5</td>\n",
       "      <td>38040.0</td>\n",
       "      <td>46799.0</td>\n",
       "      <td>84839</td>\n",
       "      <td>4819.0</td>\n",
       "      <td>8229.0</td>\n",
       "      <td>2.58</td>\n",
       "      <td>AL</td>\n",
       "      <td>Asian</td>\n",
       "      <td>4759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Rancho Cucamonga</td>\n",
       "      <td>California</td>\n",
       "      <td>34.5</td>\n",
       "      <td>88127.0</td>\n",
       "      <td>87105.0</td>\n",
       "      <td>175232</td>\n",
       "      <td>5821.0</td>\n",
       "      <td>33878.0</td>\n",
       "      <td>3.18</td>\n",
       "      <td>CA</td>\n",
       "      <td>Black or African-American</td>\n",
       "      <td>24437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Newark</td>\n",
       "      <td>New Jersey</td>\n",
       "      <td>34.6</td>\n",
       "      <td>138040.0</td>\n",
       "      <td>143873.0</td>\n",
       "      <td>281913</td>\n",
       "      <td>5829.0</td>\n",
       "      <td>86253.0</td>\n",
       "      <td>2.73</td>\n",
       "      <td>NJ</td>\n",
       "      <td>White</td>\n",
       "      <td>76402</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               City          State  Median Age  Male Population  \\\n",
       "0     Silver Spring       Maryland        33.8          40601.0   \n",
       "1            Quincy  Massachusetts        41.0          44129.0   \n",
       "2            Hoover        Alabama        38.5          38040.0   \n",
       "3  Rancho Cucamonga     California        34.5          88127.0   \n",
       "4            Newark     New Jersey        34.6         138040.0   \n",
       "\n",
       "   Female Population  Total Population  Number of Veterans  Foreign-born  \\\n",
       "0            41862.0             82463              1562.0       30908.0   \n",
       "1            49500.0             93629              4147.0       32935.0   \n",
       "2            46799.0             84839              4819.0        8229.0   \n",
       "3            87105.0            175232              5821.0       33878.0   \n",
       "4           143873.0            281913              5829.0       86253.0   \n",
       "\n",
       "   Average Household Size State Code                       Race  Count  \n",
       "0                    2.60         MD         Hispanic or Latino  25924  \n",
       "1                    2.39         MA                      White  58723  \n",
       "2                    2.58         AL                      Asian   4759  \n",
       "3                    3.18         CA  Black or African-American  24437  \n",
       "4                    2.73         NJ                      White  76402  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(df_demographics.shape)\n",
    "df_demographics.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8599212, 7)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dt</th>\n",
       "      <th>AverageTemperature</th>\n",
       "      <th>AverageTemperatureUncertainty</th>\n",
       "      <th>City</th>\n",
       "      <th>Country</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1743-11-01</td>\n",
       "      <td>6.068</td>\n",
       "      <td>1.737</td>\n",
       "      <td>Århus</td>\n",
       "      <td>Denmark</td>\n",
       "      <td>57.05N</td>\n",
       "      <td>10.33E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1743-12-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Århus</td>\n",
       "      <td>Denmark</td>\n",
       "      <td>57.05N</td>\n",
       "      <td>10.33E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1744-01-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Århus</td>\n",
       "      <td>Denmark</td>\n",
       "      <td>57.05N</td>\n",
       "      <td>10.33E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1744-02-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Århus</td>\n",
       "      <td>Denmark</td>\n",
       "      <td>57.05N</td>\n",
       "      <td>10.33E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1744-03-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Århus</td>\n",
       "      <td>Denmark</td>\n",
       "      <td>57.05N</td>\n",
       "      <td>10.33E</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           dt  AverageTemperature  AverageTemperatureUncertainty   City  \\\n",
       "0  1743-11-01               6.068                          1.737  Århus   \n",
       "1  1743-12-01                 NaN                            NaN  Århus   \n",
       "2  1744-01-01                 NaN                            NaN  Århus   \n",
       "3  1744-02-01                 NaN                            NaN  Århus   \n",
       "4  1744-03-01                 NaN                            NaN  Århus   \n",
       "\n",
       "   Country Latitude Longitude  \n",
       "0  Denmark   57.05N    10.33E  \n",
       "1  Denmark   57.05N    10.33E  \n",
       "2  Denmark   57.05N    10.33E  \n",
       "3  Denmark   57.05N    10.33E  \n",
       "4  Denmark   57.05N    10.33E  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(df_temperatures.shape)\n",
    "df_temperatures.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(55075, 12)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ident</th>\n",
       "      <th>type</th>\n",
       "      <th>name</th>\n",
       "      <th>elevation_ft</th>\n",
       "      <th>continent</th>\n",
       "      <th>iso_country</th>\n",
       "      <th>iso_region</th>\n",
       "      <th>municipality</th>\n",
       "      <th>gps_code</th>\n",
       "      <th>iata_code</th>\n",
       "      <th>local_code</th>\n",
       "      <th>coordinates</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00A</td>\n",
       "      <td>heliport</td>\n",
       "      <td>Total Rf Heliport</td>\n",
       "      <td>11.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>US</td>\n",
       "      <td>US-PA</td>\n",
       "      <td>Bensalem</td>\n",
       "      <td>00A</td>\n",
       "      <td>NaN</td>\n",
       "      <td>00A</td>\n",
       "      <td>-74.93360137939453, 40.07080078125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>00AA</td>\n",
       "      <td>small_airport</td>\n",
       "      <td>Aero B Ranch Airport</td>\n",
       "      <td>3435.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>US</td>\n",
       "      <td>US-KS</td>\n",
       "      <td>Leoti</td>\n",
       "      <td>00AA</td>\n",
       "      <td>NaN</td>\n",
       "      <td>00AA</td>\n",
       "      <td>-101.473911, 38.704022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00AK</td>\n",
       "      <td>small_airport</td>\n",
       "      <td>Lowell Field</td>\n",
       "      <td>450.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>US</td>\n",
       "      <td>US-AK</td>\n",
       "      <td>Anchor Point</td>\n",
       "      <td>00AK</td>\n",
       "      <td>NaN</td>\n",
       "      <td>00AK</td>\n",
       "      <td>-151.695999146, 59.94919968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00AL</td>\n",
       "      <td>small_airport</td>\n",
       "      <td>Epps Airpark</td>\n",
       "      <td>820.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>US</td>\n",
       "      <td>US-AL</td>\n",
       "      <td>Harvest</td>\n",
       "      <td>00AL</td>\n",
       "      <td>NaN</td>\n",
       "      <td>00AL</td>\n",
       "      <td>-86.77030181884766, 34.86479949951172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00AR</td>\n",
       "      <td>closed</td>\n",
       "      <td>Newport Hospital &amp; Clinic Heliport</td>\n",
       "      <td>237.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>US</td>\n",
       "      <td>US-AR</td>\n",
       "      <td>Newport</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-91.254898, 35.6087</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  ident           type                                name  elevation_ft  \\\n",
       "0   00A       heliport                   Total Rf Heliport          11.0   \n",
       "1  00AA  small_airport                Aero B Ranch Airport        3435.0   \n",
       "2  00AK  small_airport                        Lowell Field         450.0   \n",
       "3  00AL  small_airport                        Epps Airpark         820.0   \n",
       "4  00AR         closed  Newport Hospital & Clinic Heliport         237.0   \n",
       "\n",
       "  continent iso_country iso_region  municipality gps_code iata_code  \\\n",
       "0       NaN          US      US-PA      Bensalem      00A       NaN   \n",
       "1       NaN          US      US-KS         Leoti     00AA       NaN   \n",
       "2       NaN          US      US-AK  Anchor Point     00AK       NaN   \n",
       "3       NaN          US      US-AL       Harvest     00AL       NaN   \n",
       "4       NaN          US      US-AR       Newport      NaN       NaN   \n",
       "\n",
       "  local_code                            coordinates  \n",
       "0        00A     -74.93360137939453, 40.07080078125  \n",
       "1       00AA                 -101.473911, 38.704022  \n",
       "2       00AK            -151.695999146, 59.94919968  \n",
       "3       00AL  -86.77030181884766, 34.86479949951172  \n",
       "4        NaN                    -91.254898, 35.6087  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(df_airport_codes.shape)\n",
    "df_airport_codes.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Create Spark session to read in April 2016 immigration dataset\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.\\\n",
    "config(\"spark.jars.packages\",\"saurfang:spark-sas7bdat:2.0.0-s_2.11\")\\\n",
    ".enableHiveSupport().getOrCreate()\n",
    "df_immigration=spark.read.parquet(\"sas_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3096313\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(cicid=5748517.0, i94yr=2016.0, i94mon=4.0, i94cit=245.0, i94res=438.0, i94port='LOS', arrdate=20574.0, i94mode=1.0, i94addr='CA', depdate=20582.0, i94bir=40.0, i94visa=1.0, count=1.0, dtadfile='20160430', visapost='SYD', occup=None, entdepa='G', entdepd='O', entdepu=None, matflag='M', biryear=1976.0, dtaddto='10292016', gender='F', insnum=None, airline='QF', admnum=94953870030.0, fltno='00011', visatype='B1'),\n",
       " Row(cicid=5748518.0, i94yr=2016.0, i94mon=4.0, i94cit=245.0, i94res=438.0, i94port='LOS', arrdate=20574.0, i94mode=1.0, i94addr='NV', depdate=20591.0, i94bir=32.0, i94visa=1.0, count=1.0, dtadfile='20160430', visapost='SYD', occup=None, entdepa='G', entdepd='O', entdepu=None, matflag='M', biryear=1984.0, dtaddto='10292016', gender='F', insnum=None, airline='VA', admnum=94955622830.0, fltno='00007', visatype='B1'),\n",
       " Row(cicid=5748519.0, i94yr=2016.0, i94mon=4.0, i94cit=245.0, i94res=438.0, i94port='LOS', arrdate=20574.0, i94mode=1.0, i94addr='WA', depdate=20582.0, i94bir=29.0, i94visa=1.0, count=1.0, dtadfile='20160430', visapost='SYD', occup=None, entdepa='G', entdepd='O', entdepu=None, matflag='M', biryear=1987.0, dtaddto='10292016', gender='M', insnum=None, airline='DL', admnum=94956406530.0, fltno='00040', visatype='B1'),\n",
       " Row(cicid=5748520.0, i94yr=2016.0, i94mon=4.0, i94cit=245.0, i94res=438.0, i94port='LOS', arrdate=20574.0, i94mode=1.0, i94addr='WA', depdate=20588.0, i94bir=29.0, i94visa=1.0, count=1.0, dtadfile='20160430', visapost='SYD', occup=None, entdepa='G', entdepd='O', entdepu=None, matflag='M', biryear=1987.0, dtaddto='10292016', gender='F', insnum=None, airline='DL', admnum=94956451430.0, fltno='00040', visatype='B1'),\n",
       " Row(cicid=5748521.0, i94yr=2016.0, i94mon=4.0, i94cit=245.0, i94res=438.0, i94port='LOS', arrdate=20574.0, i94mode=1.0, i94addr='WA', depdate=20588.0, i94bir=28.0, i94visa=1.0, count=1.0, dtadfile='20160430', visapost='SYD', occup=None, entdepa='G', entdepd='O', entdepu=None, matflag='M', biryear=1988.0, dtaddto='10292016', gender='M', insnum=None, airline='DL', admnum=94956388130.0, fltno='00040', visatype='B1')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(df_immigration.count())\n",
    "df_immigration.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 2: Explore and Assess the Data\n",
    "#### Explore the Data \n",
    "Identify data quality issues, like missing values, duplicate data, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "##### **U.S. Immigration Data:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- cicid: double (nullable = true)\n",
      " |-- i94yr: double (nullable = true)\n",
      " |-- i94mon: double (nullable = true)\n",
      " |-- i94cit: double (nullable = true)\n",
      " |-- i94res: double (nullable = true)\n",
      " |-- i94port: string (nullable = true)\n",
      " |-- arrdate: double (nullable = true)\n",
      " |-- i94mode: double (nullable = true)\n",
      " |-- i94addr: string (nullable = true)\n",
      " |-- depdate: double (nullable = true)\n",
      " |-- i94bir: double (nullable = true)\n",
      " |-- i94visa: double (nullable = true)\n",
      " |-- count: double (nullable = true)\n",
      " |-- dtadfile: string (nullable = true)\n",
      " |-- visapost: string (nullable = true)\n",
      " |-- occup: string (nullable = true)\n",
      " |-- entdepa: string (nullable = true)\n",
      " |-- entdepd: string (nullable = true)\n",
      " |-- entdepu: string (nullable = true)\n",
      " |-- matflag: string (nullable = true)\n",
      " |-- biryear: double (nullable = true)\n",
      " |-- dtaddto: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- insnum: string (nullable = true)\n",
      " |-- airline: string (nullable = true)\n",
      " |-- admnum: double (nullable = true)\n",
      " |-- fltno: string (nullable = true)\n",
      " |-- visatype: string (nullable = true)\n",
      "\n",
      "+-------+------------------+-------+-------+------------------+------------------+-------+-----------------+------------------+------------------+-----------------+------------------+------------------+-------+--------------------+--------+-----------------+-------+-------+-------+-------+------------------+------------------+-------+-----------------+------------------+--------------------+------------------+--------+\n",
      "|summary|             cicid|  i94yr| i94mon|            i94cit|            i94res|i94port|          arrdate|           i94mode|           i94addr|          depdate|            i94bir|           i94visa|  count|            dtadfile|visapost|            occup|entdepa|entdepd|entdepu|matflag|           biryear|           dtaddto| gender|           insnum|           airline|              admnum|             fltno|visatype|\n",
      "+-------+------------------+-------+-------+------------------+------------------+-------+-----------------+------------------+------------------+-----------------+------------------+------------------+-------+--------------------+--------+-----------------+-------+-------+-------+-------+------------------+------------------+-------+-----------------+------------------+--------------------+------------------+--------+\n",
      "|  count|           3096313|3096313|3096313|           3096313|           3096313|3096313|          3096313|           3096074|           2943721|          2953856|           3095511|           3096313|3096313|             3096312| 1215063|             8126|3096075|2957884|    392|2957884|           3095511|           3095836|2682044|           113708|           3012686|             3096313|           3076764| 3096313|\n",
      "|   mean| 3078651.879075533| 2016.0|    4.0| 304.9069344733559|303.28381949757664|   null|20559.84854179794|1.0736897761487614|51.652482269503544|20573.95283554784|41.767614458485205|1.8453925685161674|    1.0|2.0160424766168267E7|   999.0|          885.675|   null|   null|   null|   null|1974.2323855415148| 8291120.333841449|   null|4131.050016327899|59.477601493233784|7.082885011150484E10|1360.2463696420555|    null|\n",
      "| stddev|1763278.0997499449|    0.0|    0.0|210.02688853063205|208.58321292789535|   null|8.777339475317723|0.5158963131657106| 42.97906231370983|29.35696848166157| 17.42026053458727|0.3983910200540999|    0.0|   50.01513453988029|     0.0|264.6551105950961|   null|   null|   null|   null|17.420260534589556|1656502.4244925722|   null|8821.743471773654| 172.6333995206175|2.215441594755896...| 5852.676345633695|    null|\n",
      "|    min|               6.0| 2016.0|    4.0|             101.0|             101.0|    5KE|          20545.0|               1.0|                ..|          15176.0|              -3.0|               1.0|    1.0|            20130811|     999|              049|      A|      D|      U|      M|            1902.0|          /   183D|      F|                0|               *FF|                 0.0|             00000|      B1|\n",
      "|    max|         6102785.0| 2016.0|    4.0|             999.0|             760.0|    YSL|          20574.0|               9.0|                ZU|          45427.0|             114.0|               3.0|    1.0|            20160919|     ZZZ|              WTR|      Z|      W|      Y|      M|            2019.0|               D/S|      X|           YM0167|                ZZ|      9.991556593E10|               ZZZ|      WT|\n",
      "+-------+------------------+-------+-------+------------------+------------------+-------+-----------------+------------------+------------------+-----------------+------------------+------------------+-------+--------------------+--------+-----------------+-------+-------+-------+-------+------------------+------------------+-------+-----------------+------------------+--------------------+------------------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Immigration data\n",
    "df_immigration.printSchema()\n",
    "df_immigration.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "- Data Dictionary (from I94_SAS_Labels_Descriptions.SAS):\n",
    "\n",
    "| Column Name | Description |\n",
    "|-----|-----|\n",
    "| cicid | Unique identifier |\n",
    "| i94yr | 4 digit year of entry |\n",
    "| i94mon | Numeric month of entry |\n",
    "| i94cit | Country of citizenship (list of valid and invalid 3-digit country codes) |\n",
    "| i94res | Country of residence (list of valid and invalid 3-digit country codes) |\n",
    "| i94port | U.S. port of entry (list of valid and invalid 3-letter city/town codes) |\n",
    "| arrdate | Arrival Date in the USA. It is a SAS date numeric field. |\n",
    "| i94mode | Mode of transport: 1 = 'Air', 2 = 'Sea', 3 = 'Land', 9 = 'Not reported' |\n",
    "| i94addr | Arrival address (list of valid and invalid 2-letter state codes) |\n",
    "| depdate | Departure Date from the USA. It is a SAS date numeric field. |\n",
    "| i94bir | Age of Respondent in Years |\n",
    "| i94visa | Visa codes collapsed into three categories: 1 = Business, 2 = Pleasure, 3 = Student |\n",
    "| count | Used for summary statistics |\n",
    "| dtadfile |  Character Date Field - Date added to I-94 Files - CIC does not use |\n",
    "| visapost | Department of State where where Visa was issued - CIC does not use |\n",
    "| occup | Occupation that will be performed in U.S. - CIC does not use |\n",
    "| entdepa | Arrival Flag - admitted or paroled into the U.S. - CIC does not use |\n",
    "| entdepd | Departure Flag - Departed, lost I-94 or is deceased - CIC does not use | \n",
    "| entdepu | Update Flag - Either apprehended, overstayed, adjusted to perm residence - CIC does not use | \n",
    "| matflag | Match flag - Match of arrival and departure records | \n",
    "| biryear | 4 digit year of birth | \n",
    "| dtaddto | Character Date Field - Date to which admitted to U.S. (allowed to stay until) - CIC does not use | \n",
    "| gender | Non-immigrant sex  | \n",
    "| insnum | INS number | \n",
    "| airline | Airline used to arrive in U.S. | \n",
    "| admnum | Admission Number |\n",
    "| fltno | Flight number of Airline used to arrive in U.S. |\n",
    "| visatype | Class of admission legally admitting the non-immigrant to temporarily stay in U.S. |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "- Observations:\n",
    "  - Fields with a high number of missing values include: visapost, occup, entdepu, and insnum. These fields should be discarded.\n",
    "  - Fields not providing useful information for our purposes include: count, dtadvile, visapost, entdepa, entdepd, entdepu, matflag, dtaddto, insnum, and fltno. These fields should be discarded.\n",
    "  - Ages (i94bir) range from -3 to 114. These records should be filtered to exclude ages < 0 or >100, which can be assumed to be input errors.\n",
    "  - The dates are stored in SAS date format (days since January 1, 1960). These dates should be converted YYYY-MM-DD string format."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "##### **U.S. City Demographics Data:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2891 entries, 0 to 2890\n",
      "Data columns (total 12 columns):\n",
      "City                      2891 non-null object\n",
      "State                     2891 non-null object\n",
      "Median Age                2891 non-null float64\n",
      "Male Population           2888 non-null float64\n",
      "Female Population         2888 non-null float64\n",
      "Total Population          2891 non-null int64\n",
      "Number of Veterans        2878 non-null float64\n",
      "Foreign-born              2878 non-null float64\n",
      "Average Household Size    2875 non-null float64\n",
      "State Code                2891 non-null object\n",
      "Race                      2891 non-null object\n",
      "Count                     2891 non-null int64\n",
      "dtypes: float64(6), int64(2), object(4)\n",
      "memory usage: 271.1+ KB\n"
     ]
    }
   ],
   "source": [
    "# City demographics data\n",
    "df_demographics.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "- Data Dictionary:\n",
    "\n",
    "| Column Name | Description |\n",
    "|-----|-----|\n",
    "| City | Name of city |\n",
    "| State | Name of state |\n",
    "| Median Age | Median age of city population |\n",
    "| Male Population | Number of males living in city |\n",
    "| Female Population | Number of females living in city |\n",
    "| Total Population | Total number of people living in city |\n",
    "| Number of Veterans | Number of veterans living in city |\n",
    "| Foreign-born | Number of non-U.S. born people living in city |\n",
    "| Average Household Size | Average number of people per household in city |\n",
    "| State Code | Code of state |\n",
    "| Race | Race class of people living in city |\n",
    "| Count | Number of people in race class living in city |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "- Observations:\n",
    "  - There are no fields that contain a significant number of missing values.\n",
    "  - For this particular project, the decision has been made to not use this dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "##### **World Temperature Data:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 8599212 entries, 0 to 8599211\n",
      "Data columns (total 7 columns):\n",
      "dt                               object\n",
      "AverageTemperature               float64\n",
      "AverageTemperatureUncertainty    float64\n",
      "City                             object\n",
      "Country                          object\n",
      "Latitude                         object\n",
      "Longitude                        object\n",
      "dtypes: float64(2), object(5)\n",
      "memory usage: 459.2+ MB\n"
     ]
    }
   ],
   "source": [
    "# Temperature data\n",
    "df_temperatures.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "- Data Dictionary:\n",
    "\n",
    "| Column Name | Description |\n",
    "|-----|-----|\n",
    "| dt | Date of temperature reading (YYYY-MM-DD) |\n",
    "| AverageTemperature | Average temperature in city on date |\n",
    "| AverageTemperatureUncertainty | Uncertainty measure of average temperature |\n",
    "| City | Name of city |\n",
    "| Country | Number of country |\n",
    "| Latitude | Latitude of city |\n",
    "| Longitude | Longitude of city |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "- Observations:\n",
    "  - Fields not providing useful information for our purposes include: AverageTemperatureUncertainty, Latitude, and Longitude. These fields should be discarded.\n",
    "  - For this particular project, the decision has been made to not use this dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "##### **Global Airport Codes Data:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 55075 entries, 0 to 55074\n",
      "Data columns (total 12 columns):\n",
      "ident           55075 non-null object\n",
      "type            55075 non-null object\n",
      "name            55075 non-null object\n",
      "elevation_ft    48069 non-null float64\n",
      "continent       27356 non-null object\n",
      "iso_country     54828 non-null object\n",
      "iso_region      55075 non-null object\n",
      "municipality    49399 non-null object\n",
      "gps_code        41030 non-null object\n",
      "iata_code       9189 non-null object\n",
      "local_code      28686 non-null object\n",
      "coordinates     55075 non-null object\n",
      "dtypes: float64(1), object(11)\n",
      "memory usage: 5.0+ MB\n"
     ]
    }
   ],
   "source": [
    "# Airport codes data\n",
    "df_airport_codes.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "- Data Dictionary:\n",
    "\n",
    "| Column Name | Description |\n",
    "|-----|-----|\n",
    "| ident | Unique identifier |\n",
    "| type | Type of airport |\n",
    "| name | Name of airport |\n",
    "| elevation_ft | Altitude of airport (feet) |\n",
    "| continent | Continent of airport |\n",
    "| iso_country | ISO country code of airport (2 letters) |\n",
    "| iso_region | ISO region code of airport (4 letters) |\n",
    "| municipality | City of airport |\n",
    "| gps_code | GPS code for airport |\n",
    "| iata_code | IATA code for airport (3 letters) |\n",
    "| local_code | ICAO code for airport (4 letters)|\n",
    "| coordinates | GPS coordinates for airport |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "- Observations:\n",
    "  - Fields with a high number of missing values include: continent, iata_code, and local_code. \n",
    "  - Fields not providing useful information for our purposes include: type, elevation_ft, continent, iso_country, iso_region, municality, gps_code, and coordinates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Cleaning Steps\n",
    "\n",
    "- U.S. Immigration Data:\n",
    "  - Drop records if duplicates exist.\n",
    "  - Drop records with NaN values in key columns.\n",
    "- U.S. City Demographics Data:\n",
    "  - Drop duplicate records.\n",
    "  - Drop records with NaN values.\n",
    "- World Temperature Data:\n",
    "  - Drop duplicate records.\n",
    "  - Drop records with NaN values in key columns.\n",
    "- Global Airport Codes Data:\n",
    "  - Drop duplicate records.\n",
    "  - Drop records with NaN values in key columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Clean U.S. immigration data\n",
    "\n",
    "def clean_immig(df_immig):\n",
    "    '''\n",
    "    Performs cleaning tasks on U.S. immigration dataset\n",
    "    Input: original dataframe\n",
    "    Output: transformed dataframe\n",
    "    '''\n",
    "    # Drop records if duplicates exist\n",
    "    if df_immig.count() != df_immig.dropDuplicates().count():\n",
    "        df_immig.dropDuplicates()\n",
    "        \n",
    "    # Drop records with NaN values in key columns\n",
    "    df_immig = df_immig.dropna(subset=['i94yr', 'i94mon', 'i94res','i94port', 'arrdate', 'i94bir', 'gender', 'admnum'])\n",
    "    \n",
    "    return df_immig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Clean U.S. city demographics data\n",
    "\n",
    "def clean_demo(df_demo):\n",
    "    '''\n",
    "    Performs cleaning tasks on U.S. city demographics dataset\n",
    "    Input: original dataframe\n",
    "    Output: transformed dataframe\n",
    "    '''    \n",
    "    # Drop duplicate records\n",
    "    df_demo.drop_duplicates()\n",
    "    \n",
    "    # Drop records with NaN values\n",
    "    df_demo.dropna()\n",
    "    \n",
    "    return df_demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Clean world temperature data\n",
    "\n",
    "def clean_temp(df_temp):\n",
    "    '''\n",
    "    Performs cleaning tasks on world temperature dataset\n",
    "    Input: original dataframe\n",
    "    Output: transformed dataframe\n",
    "    '''\n",
    "    # Drop duplicate records\n",
    "    df_temp.drop_duplicates()\n",
    "    \n",
    "    # Drop records with NaN values in key columns\n",
    "    df_temp = df_temp.dropna(subset=['dt', 'AverageTemperature', 'City', 'Country'])\n",
    "    \n",
    "    return df_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Clean global airport data\n",
    "\n",
    "def clean_airport(df_airport):\n",
    "    '''\n",
    "    Performs cleaning tasks on global airport codes dataset\n",
    "    Input: original dataframe\n",
    "    Output: transformed dataframe\n",
    "    '''\n",
    "    # Drop duplicate records\n",
    "    df_airport.dropDuplicates()\n",
    "    \n",
    "    # Drop records with NaN values in key columns\n",
    "    df_airport = df_airport.dropna(subset=['ident', 'name', 'iso_region'])\n",
    "    \n",
    "    return df_airport"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 3: Define the Data Model\n",
    "#### 3.1 Conceptual Data Model\n",
    "\n",
    "The data model is built around a specific business process, the decision by U.S. Immigration to admit visitors into the United States of America. This process generates immigration events and related performance metrics.\n",
    "\n",
    "The STAR schema design, one of the most widely-used schemas in the industry, is also the simplest style of data mart schema. It was selected for this situation because, compared to other potential choices (e.g. SNOWFLAKE Schema):\n",
    "1. It is more effective at handling simplified queries. As this data mart has just been created, it is likely that U.S. Immigration BI user needs will be basic. \n",
    "2. It supports fast aggregations. When simple queries are used, performance can be improved, which is important given a potential large dataset of historical immigration activity.\n",
    "3. It supports simplified reporting. Business reporting logic, such as period-over-period and as-of reporting, is generally much simpler for the star schema.\n",
    "\n",
    "Dimension tables should provide information on the *who, what, where, when, why, and how* related to the U.S. immigration process. Therefore, the following dimension tables have been built:\n",
    "- **dim_immigrant** provides information on *who* is entering the U.S.\n",
    "- **dim_aiport** provides information on *how* this person entered the U.S.\n",
    "- **dim_country** provides information on *where* this person originates.\n",
    "- **dim_time** provides information on *when* this person entered the U.S.\n",
    "\n",
    "The fact table should contain measurements and metrics related to the U.S. immigration process. Here, the following fact table has been built:\n",
    "- **fact_immigration** provides the measurements and metrics related to the U.S. immigration process, e.g. how many people entered the U.S.?\n",
    "\n",
    "Following is an Entity Relationship Diagram (ERD) depicting the STAR schema used for this project:\n",
    "![data model](data_model.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 3.2 Mapping Out Data Pipelines\n",
    "List of steps necessary to pipeline the data into the chosen data model:\n",
    "1. Extract U.S. immigration data to create a Spark dataframe.\n",
    "2. Extract Global airport code data to create Pandas dataframe.\n",
    "3. Clean immigration and airport data by removing duplicates and NaN records.\n",
    "4. Create dimension tables (airport, country, immigrant, time) and write to parquet files.\n",
    "5. Create a fact table (immigration) and write to parquet file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 4: Run Pipelines to Model the Data \n",
    "#### 4.1 Create the data model\n",
    "Build the data pipelines to create the data model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Import required modules and libraries\n",
    "import os\n",
    "import glob\n",
    "import json\n",
    "import pandas as pd\n",
    "import pyspark.sql.functions as f\n",
    "from datetime import datetime, timedelta, date\n",
    "from pyspark.sql.types import StringType\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import types as t\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.functions import year, month, dayofmonth, hour, weekofyear"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 4.1.1 Extract and Clean Immigration Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Create Spark session\n",
    "spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .config('spark.jars.packages','saurfang:spark-sas7bdat:2.0.0-s_2.11') \\\n",
    "        .enableHiveSupport().getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Load immmigration labels\n",
    "#dir_path = os.path.dirname(os.path.realpath(__file__))\n",
    "#immig_labels = json.load(open(dir_path + '/immigration_labels.json'))\n",
    "immig_labels = json.load(open('./immigration_labels.json'))\n",
    "\n",
    "country_code = immig_labels['country_code']\n",
    "city_code = immig_labels['city_code']\n",
    "travel_code = immig_labels['travel_code']\n",
    "state_list = immig_labels['state_code']\n",
    "state_code = {state: code for code, state in state_list.items()}\n",
    "visa_code = immig_labels['visa_code']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Create user defined functions for codes\n",
    "country_udf = udf(lambda country: country_code[country], StringType())\n",
    "city_udf = udf(lambda city: city_code[city], StringType())\n",
    "travel_udf = udf(lambda mode: travel_code[mode], StringType())\n",
    "state_udf = udf(lambda state: state_code[state], StringType())\n",
    "visa_udf = udf(lambda visa: visa_code[visa], StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Get filepath to immigration source and staging data files\n",
    "immigration_source = '../../data/18-83510-I94-Data-2016/i94_apr16_sub.sas7bdat'\n",
    "immigration_staging = 'staging_immigration'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Read immigration data file\n",
    "df_immigration = spark.read.format('com.github.saurfang.sas.spark').load(immigration_source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Clean immigration data file\n",
    "df_immigration_clean = clean_immig(df_immigration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Define timestamp column\n",
    "@udf(t.TimestampType())\n",
    "def get_timestamp (arrdate):\n",
    "    arrdate_int = int(arrdate)\n",
    "    return (datetime(1960,1,1) + timedelta(days=arrdate_int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Create staging file against which SQL queries can be run\n",
    "df_immigration_stage = df_immigration_clean.filter(df_immigration_clean.i94addr.isNotNull())\\\n",
    "    .filter(df_immigration_clean.i94res.isNotNull())\\\n",
    "    .filter(f.col(\"i94addr\").isin(list(state_code.keys())))\\\n",
    "    .filter(f.col(\"i94port\").isin(list(city_code.keys())))\\\n",
    "    .withColumn('immigration_id', f.col('cicid').cast('integer'))\\\n",
    "    .withColumn('country_code', f.col('i94res').cast('integer').cast('string'))\\\n",
    "    .withColumn('arrival_date', f.col('arrdate'))\\\n",
    "    .withColumn('arrival_time', get_timestamp('arrival_date'))\\\n",
    "    .withColumn('country', country_udf(f.col('country_code')))\\\n",
    "    .withColumn('state_code', f.col('i94addr'))\\\n",
    "    .withColumn('state', state_udf(f.col('i94addr')))\\\n",
    "    .withColumn('airport_id', f.col('i94port'))\\\n",
    "    .withColumn('city', city_udf(f.col('i94port')))\\\n",
    "    .withColumn('arrival_year', f.col('i94yr').cast('integer'))\\\n",
    "    .withColumn('arrival_month', f.col('i94mon').cast('integer'))\\\n",
    "    .withColumn('arrival_mode_code', f.col('i94mode').cast('integer').cast('string'))\\\n",
    "    .withColumn('arrival_mode', travel_udf(f.col('arrival_mode_code')))\\\n",
    "    .withColumn('age', f.col('i94bir').cast('integer'))\\\n",
    "    .withColumn('visa_category_code', f.col('i94visa').cast('integer').cast('string'))\\\n",
    "    .withColumn('visa_category', visa_udf(f.col('visa_category_code')))\\\n",
    "    .withColumn('gender', f.col('gender'))\\\n",
    "    .withColumn('admission_id', f.col('admnum'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Write staging file to parquet files\n",
    "df_immigration_stage.select('immigration_id', 'arrival_date', 'arrival_time', 'country_code', 'country', 'state_code', \n",
    "                            'state', 'airport_id', 'city', 'arrival_year', 'arrival_month', 'arrival_mode_code', \n",
    "                            'arrival_mode', 'age', 'visa_category_code', 'visa_category', 'gender', 'admission_id')\\\n",
    "                    .write.mode('overwrite')\\\n",
    "                    .option('compression', 'gzip')\\\n",
    "                    .parquet(immigration_staging)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 4.1.2 Extract and Clean Airport Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Get filepath to airport source and staging data files\n",
    "airport_source = './airport-codes_csv.csv'\n",
    "airport_staging = 'staging_airport'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Read airport data file\n",
    "df_airport_codes = spark.read.csv(airport_source, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Clean airport data file\n",
    "df_airport_codes_clean = clean_airport(df_airport_codes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Replace iso_region with state code\n",
    "split_col = f.split(df_airport_codes_clean['iso_region'], '-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Create staging file against which SQL queries can be run\n",
    "df_airport_stage = df_airport_codes_clean\\\n",
    "    .withColumn('airport_id', f.col('ident'))\\\n",
    "    .withColumn('airport_type', f.col('type'))\\\n",
    "    .withColumn('airport', f.col('name'))\\\n",
    "    .withColumn('state_code', split_col.getItem(1))\\\n",
    "    .withColumn('city', f.col('municipality'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Write staging file to parquet files\n",
    "df_airport_stage.select('airport_id', 'airport_type', 'airport', 'state_code', 'city')\\\n",
    "                    .write.mode('overwrite')\\\n",
    "                    .option('compression', 'gzip')\\\n",
    "                    .parquet(airport_staging)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 4.1.3 Create dimension tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Read immigration staging parquet files\n",
    "df_immigration = spark.read.parquet(immigration_staging)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Create a temporary view against which SQL queries can be run\n",
    "df_immigration.createOrReplaceTempView('immigrant')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Create immigrant dimension table\n",
    "dim_immigrant = spark.sql(\"\"\"\n",
    "    SELECT DISTINCT\n",
    "        immigrant.admission_id,\n",
    "        immigrant.country_code,\n",
    "        immigrant.age,\n",
    "        immigrant.gender,\n",
    "        immigrant.visa_category,\n",
    "        immigrant.visa_category_code\n",
    "    FROM immigrant\n",
    "    ORDER BY 1,2\n",
    "\"\"\").cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- admission_id: double (nullable = true)\n",
      " |-- country_code: string (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- visa_category: string (nullable = true)\n",
      " |-- visa_category_code: string (nullable = true)\n",
      "\n",
      "+------------+------------+---+------+-------------+------------------+\n",
      "|admission_id|country_code|age|gender|visa_category|visa_category_code|\n",
      "+------------+------------+---+------+-------------+------------------+\n",
      "|         0.0|         111| 40|     F|     Pleasure|                 2|\n",
      "|         0.0|         113| 39|     M|     Pleasure|                 2|\n",
      "|         0.0|         135| 43|     M|     Pleasure|                 2|\n",
      "|         0.0|         464| 19|     F|     Pleasure|                 2|\n",
      "|         0.0|         582| 54|     M|     Pleasure|                 2|\n",
      "|         0.0|         689| 66|     F|     Pleasure|                 2|\n",
      "|         0.0|         689| 67|     F|     Pleasure|                 2|\n",
      "|   1218224.0|         585| 59|     M|     Pleasure|                 2|\n",
      "|   1219424.0|         528| 37|     M|     Pleasure|                 2|\n",
      "|   1222424.0|         692| 16|     F|     Pleasure|                 2|\n",
      "|   1226124.0|         584| 48|     F|     Pleasure|                 2|\n",
      "|   1226224.0|         584| 70|     F|     Pleasure|                 2|\n",
      "|   1236624.0|         687| 49|     X|     Pleasure|                 2|\n",
      "|   1236724.0|         687| 46|     F|     Pleasure|                 2|\n",
      "|   1236824.0|         687|  2|     M|     Pleasure|                 2|\n",
      "|   1236924.0|         687| 12|     M|     Pleasure|                 2|\n",
      "|   1237124.0|         687| 10|     F|     Pleasure|                 2|\n",
      "|   1246424.0|         528| 20|     M|     Pleasure|                 2|\n",
      "|   1248124.0|         696| 31|     F|     Pleasure|                 2|\n",
      "|   1248324.0|         696| 41|     M|     Pleasure|                 2|\n",
      "+------------+------------+---+------+-------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Inspect immigrant dimension table schema and first 20 rows\n",
    "dim_immigrant.printSchema()\n",
    "dim_immigrant.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Write immigrant table to parquet file\n",
    "dim_immigrant.write.mode(\"overwrite\").parquet('dim_immigrant')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Create a temporary view against which SQL queries can be run\n",
    "df_immigration.createOrReplaceTempView('country')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Create country dimension table\n",
    "dim_country = spark.sql(\"\"\"\n",
    "    SELECT DISTINCT\n",
    "        country.country_code,\n",
    "        country.country\n",
    "    FROM country\n",
    "    ORDER BY 1\n",
    "\"\"\").cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- country_code: string (nullable = true)\n",
      " |-- country: string (nullable = true)\n",
      "\n",
      "+------------+-------------+\n",
      "|country_code|      country|\n",
      "+------------+-------------+\n",
      "|         101|      ALBANIA|\n",
      "|         102|      ANDORRA|\n",
      "|         103|      AUSTRIA|\n",
      "|         104|      BELGIUM|\n",
      "|         105|     BULGARIA|\n",
      "|         107|       POLAND|\n",
      "|         108|      DENMARK|\n",
      "|         109|      ESTONIA|\n",
      "|         110|      FINLAND|\n",
      "|         111|       FRANCE|\n",
      "|         112|      GERMANY|\n",
      "|         113|       GREECE|\n",
      "|         114|      HUNGARY|\n",
      "|         115|      ICELAND|\n",
      "|         116|      IRELAND|\n",
      "|         117|        ITALY|\n",
      "|         118|       LATVIA|\n",
      "|         119|LIECHTENSTEIN|\n",
      "|         120|    LITHUANIA|\n",
      "|         121|   LUXEMBOURG|\n",
      "+------------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Inspect country dimension table schema and first 20 rows\n",
    "dim_country.printSchema()\n",
    "dim_country.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Write country table to parquet file\n",
    "dim_country.write.mode(\"overwrite\").parquet('dim_country')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Create a temporary view against which SQL queries can be run\n",
    "df_immigration.createOrReplaceTempView('time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Create time dimension table\n",
    "dim_time = spark.sql(\"\"\"\n",
    "    SELECT DISTINCT\n",
    "        time.arrival_time  AS arrival_ts, \n",
    "        hour(arrival_time)       AS hour, \n",
    "        day(arrival_time)        AS day, \n",
    "        weekofyear(arrival_time) AS week,\n",
    "        month(arrival_time)      AS month,\n",
    "        year(arrival_time)       AS year,\n",
    "        dayofweek(arrival_time)  AS weekday\n",
    "    FROM time\n",
    "\"\"\").cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- arrival_ts: timestamp (nullable = true)\n",
      " |-- hour: integer (nullable = true)\n",
      " |-- day: integer (nullable = true)\n",
      " |-- week: integer (nullable = true)\n",
      " |-- month: integer (nullable = true)\n",
      " |-- year: integer (nullable = true)\n",
      " |-- weekday: integer (nullable = true)\n",
      "\n",
      "+-------------------+----+---+----+-----+----+-------+\n",
      "|         arrival_ts|hour|day|week|month|year|weekday|\n",
      "+-------------------+----+---+----+-----+----+-------+\n",
      "|2016-04-02 00:00:00|   0|  2|  13|    4|2016|      7|\n",
      "|2016-04-23 00:00:00|   0| 23|  16|    4|2016|      7|\n",
      "|2016-04-22 00:00:00|   0| 22|  16|    4|2016|      6|\n",
      "|2016-04-11 00:00:00|   0| 11|  15|    4|2016|      2|\n",
      "|2016-04-12 00:00:00|   0| 12|  15|    4|2016|      3|\n",
      "|2016-04-24 00:00:00|   0| 24|  16|    4|2016|      1|\n",
      "|2016-04-17 00:00:00|   0| 17|  15|    4|2016|      1|\n",
      "|2016-04-06 00:00:00|   0|  6|  14|    4|2016|      4|\n",
      "|2016-04-27 00:00:00|   0| 27|  17|    4|2016|      4|\n",
      "|2016-04-09 00:00:00|   0|  9|  14|    4|2016|      7|\n",
      "|2016-04-28 00:00:00|   0| 28|  17|    4|2016|      5|\n",
      "|2016-04-30 00:00:00|   0| 30|  17|    4|2016|      7|\n",
      "|2016-04-21 00:00:00|   0| 21|  16|    4|2016|      5|\n",
      "|2016-04-16 00:00:00|   0| 16|  15|    4|2016|      7|\n",
      "|2016-04-07 00:00:00|   0|  7|  14|    4|2016|      5|\n",
      "|2016-04-08 00:00:00|   0|  8|  14|    4|2016|      6|\n",
      "|2016-04-25 00:00:00|   0| 25|  17|    4|2016|      2|\n",
      "|2016-04-20 00:00:00|   0| 20|  16|    4|2016|      4|\n",
      "|2016-04-03 00:00:00|   0|  3|  13|    4|2016|      1|\n",
      "|2016-04-19 00:00:00|   0| 19|  16|    4|2016|      3|\n",
      "+-------------------+----+---+----+-----+----+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Inspect time dimension table schema and first 20 rows\n",
    "dim_time.printSchema()\n",
    "dim_time.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Write time table to parquet file\n",
    "dim_time.write.mode(\"overwrite\").parquet('dim_time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Read airport staging parquet files\n",
    "df_airport = spark.read.parquet(airport_staging)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Create a temporary view against which SQL queries can be run\n",
    "df_airport.createOrReplaceTempView('airport')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Create airport dimension table\n",
    "dim_airport = spark.sql(\"\"\"\n",
    "    SELECT DISTINCT\n",
    "        airport.airport_id, \n",
    "        airport.airport_type, \n",
    "        airport.airport, \n",
    "        airport.state_code, \n",
    "        airport.city \n",
    "    FROM airport\n",
    "\"\"\").cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- airport_id: string (nullable = true)\n",
      " |-- airport_type: string (nullable = true)\n",
      " |-- airport: string (nullable = true)\n",
      " |-- state_code: string (nullable = true)\n",
      " |-- city: string (nullable = true)\n",
      "\n",
      "+----------+-------------+--------------------+----------+--------------------+\n",
      "|airport_id| airport_type|             airport|state_code|                city|\n",
      "+----------+-------------+--------------------+----------+--------------------+\n",
      "|      0AK1|small_airport|Anderson Lake Air...|        AK|             Wasilla|\n",
      "|      0GE5|small_airport|    Mountain Airpark|        GA|           Cleveland|\n",
      "|      0IS8|     heliport|Blessing Hospital...|        IL|              Quincy|\n",
      "|       0N4|small_airport|Chandelle Estates...|        DE|               Dover|\n",
      "|      0NC5|     heliport|Nash General Hosp...|        NC|         Rocky Mount|\n",
      "|      0NC6|     heliport|Our Community Hos...|        NC|       Scotland Neck|\n",
      "|      0OR4|     heliport|Round Butte Heliport|        OR|            Metolius|\n",
      "|      0WN4|     heliport|    Olympia Heliport|        WA|             Olympia|\n",
      "|      11CO|       closed|  Channel 7 Heliport|        CO|              Denver|\n",
      "|       11G|       closed|       Johnson Field|        MI|        Smiths Creek|\n",
      "|      17LL|small_airport|  Oink Acres Airport|        IL|             Mahomet|\n",
      "|      17MN|seaplane_base|Jackson Seaplane ...|        MN|           Mc Gregor|\n",
      "|      1ID8|     heliport|Gooding County Me...|        ID|             Gooding|\n",
      "|      1MD8|small_airport|Mayberry Run Airport|        MD|Westminster/Silve...|\n",
      "|      1OI6|small_airport|       Stone Airport|        OH|         New Lebanon|\n",
      "|      27OI|       closed|      Auburn Airport|        OH|       Chagrin Falls|\n",
      "|      28PN|     heliport|     Harsco Heliport|        PA|        Wormleysburg|\n",
      "|      2AZ6|     heliport|    Horizon Heliport|        AZ|          Oro Valley|\n",
      "|       2C6|small_airport|  Tri-County Airport|        IL|          Yates City|\n",
      "|      2ID4|       closed|  Silverwood Airport|        ID|               Athol|\n",
      "+----------+-------------+--------------------+----------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Inspect airport dimension table schema and first 20 rows\n",
    "dim_airport.printSchema()\n",
    "dim_airport.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Write airport table to parquet file\n",
    "dim_airport.write.mode(\"overwrite\").parquet('dim_airport')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 4.1.4 Create fact table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Create a temporary view against which SQL queries can be run\n",
    "df_immigration.createOrReplaceTempView('immigration')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- immigration_id: integer (nullable = true)\n",
      " |-- arrival_date: double (nullable = true)\n",
      " |-- arrival_time: timestamp (nullable = true)\n",
      " |-- country_code: string (nullable = true)\n",
      " |-- country: string (nullable = true)\n",
      " |-- state_code: string (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      " |-- airport_id: string (nullable = true)\n",
      " |-- city: string (nullable = true)\n",
      " |-- arrival_year: integer (nullable = true)\n",
      " |-- arrival_month: integer (nullable = true)\n",
      " |-- arrival_mode_code: string (nullable = true)\n",
      " |-- arrival_mode: string (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      " |-- visa_category_code: string (nullable = true)\n",
      " |-- visa_category: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- admission_id: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Inspect fact table schema and first 20 rows\n",
    "df_immigration.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Create fact table\n",
    "fact_immigration = spark.sql(\"\"\"\n",
    "    SELECT DISTINCT\n",
    "        immigration.immigration_id,\n",
    "        immigration.arrival_time,\n",
    "        immigration.admission_id,\n",
    "        immigration.airport_id,\n",
    "        immigration.country_code,\n",
    "        immigration.arrival_mode,\n",
    "        immigration.arrival_year,\n",
    "        immigration.arrival_month\n",
    "    FROM immigration\n",
    "\"\"\").cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- immigration_id: integer (nullable = true)\n",
      " |-- arrival_time: timestamp (nullable = true)\n",
      " |-- admission_id: double (nullable = true)\n",
      " |-- airport_id: string (nullable = true)\n",
      " |-- country_code: string (nullable = true)\n",
      " |-- arrival_mode: string (nullable = true)\n",
      " |-- arrival_year: integer (nullable = true)\n",
      " |-- arrival_month: integer (nullable = true)\n",
      "\n",
      "+--------------+-------------------+---------------+----------+------------+------------+------------+-------------+\n",
      "|immigration_id|       arrival_time|   admission_id|airport_id|country_code|arrival_mode|arrival_year|arrival_month|\n",
      "+--------------+-------------------+---------------+----------+------------+------------+------------+-------------+\n",
      "|            31|2016-04-01 00:00:00| 9.247128923E10|       ATL|         101|         Air|        2016|            4|\n",
      "|           262|2016-04-01 00:00:00|5.5436887033E10|       NYC|         103|         Air|        2016|            4|\n",
      "|           378|2016-04-01 00:00:00|5.5429227433E10|       MIA|         103|         Air|        2016|            4|\n",
      "|           919|2016-04-01 00:00:00|5.5436331633E10|       NEW|         104|         Air|        2016|            4|\n",
      "|          1691|2016-04-01 00:00:00|5.5460699333E10|       MIA|         104|         Air|        2016|            4|\n",
      "|          1831|2016-04-01 00:00:00|5.5456188433E10|       SFB|         104|         Air|        2016|            4|\n",
      "|          1975|2016-04-01 00:00:00|5.5404025533E10|       NYC|         296|         Air|        2016|            4|\n",
      "|          1980|2016-04-01 00:00:00| 9.251349903E10|       ATL|         104|         Air|        2016|            4|\n",
      "|          2213|2016-04-01 00:00:00| 9.248182183E10|       SLC|         105|         Air|        2016|            4|\n",
      "|          2214|2016-04-01 00:00:00| 9.248196483E10|       SLC|         105|         Air|        2016|            4|\n",
      "|          2465|2016-04-01 00:00:00| 9.248147523E10|       POR|         107|        Land|        2016|            4|\n",
      "|          3123|2016-04-01 00:00:00|5.5463038733E10|       NYC|         108|         Air|        2016|            4|\n",
      "|          3359|2016-04-01 00:00:00|5.5453127733E10|       CHI|         108|         Air|        2016|            4|\n",
      "|          4172|2016-04-01 00:00:00|5.5431202933E10|       LOS|         110|         Air|        2016|            4|\n",
      "|          4260|2016-04-01 00:00:00|5.5421830933E10|       SEA|         110|         Air|        2016|            4|\n",
      "|          4344|2016-04-01 00:00:00|5.5428717833E10|       NEW|         130|         Air|        2016|            4|\n",
      "|          4480|2016-04-01 00:00:00|5.5461676533E10|       ATL|         111|         Air|        2016|            4|\n",
      "|          4706|2016-04-01 00:00:00|   6.67345385E8|       BAL|         111|         Air|        2016|            4|\n",
      "|          5207|2016-04-01 00:00:00|5.5413416833E10|       NEW|         111|         Air|        2016|            4|\n",
      "|          5488|2016-04-01 00:00:00|5.5441775733E10|       NEW|         111|         Air|        2016|            4|\n",
      "+--------------+-------------------+---------------+----------+------------+------------+------------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Inspect fact table schema and first 20 rows\n",
    "fact_immigration.printSchema()\n",
    "fact_immigration.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Write immigration table to parquet file\n",
    "fact_immigration.write.mode(\"overwrite\").parquet('fact_immigration')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 4.2 Data Quality Checks\n",
    "Explain the data quality checks you'll perform to ensure the pipeline ran as expected. These could include:\n",
    " * Integrity constraints on the relational database (e.g., unique key, data type, etc.)\n",
    " * Unit tests for the scripts to ensure they are doing the right thing\n",
    " * Source/Count checks to ensure completeness\n",
    " \n",
    "Run Quality Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Load tables to be checked\n",
    "fact_immigration = spark.read.parquet('fact_immigration')\n",
    "dim_immigrant = spark.read.parquet('dim_immigrant')\n",
    "dim_country = spark.read.parquet('dim_country')\n",
    "dim_time = spark.read.parquet('dim_time')\n",
    "dim_airport = spark.read.parquet('dim_airport')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Define zero record check\n",
    "def zero_check(df, description):\n",
    "    '''\n",
    "    Input: Spark fact or dimension tables\n",
    "    Output: Outcome of quality check\n",
    "    '''\n",
    "    result = df.count()\n",
    "    if result == 0:\n",
    "        print(\"Data quality check failed for {} with zero records\".format(description))\n",
    "    else:\n",
    "        print(\"Data quality check passed for {} with {} records\".format(description, result))\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data quality check passed for immigration fact table with 2441158 records\n",
      "Data quality check passed for immigrant dimension table with 2430721 records\n",
      "Data quality check passed for country dimension table with 228 records\n",
      "Data quality check passed for time dimension table with 30 records\n",
      "Data quality check passed for airport dimension table with 55075 records\n"
     ]
    }
   ],
   "source": [
    "# Perform zero record check\n",
    "zero_check(fact_immigration, 'immigration fact table')\n",
    "zero_check(dim_immigrant, 'immigrant dimension table')\n",
    "zero_check(dim_country, 'country dimension table')\n",
    "zero_check(dim_time, 'time dimension table')\n",
    "zero_check(dim_airport, 'airport dimension table')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------+----------------------+----------------------+--------------------+----------------------+----------------------+----------------------+-----------------------+\n",
      "|(immigration_id IS NULL)|(arrival_time IS NULL)|(admission_id IS NULL)|(airport_id IS NULL)|(country_code IS NULL)|(arrival_mode IS NULL)|(arrival_year IS NULL)|(arrival_month IS NULL)|\n",
      "+------------------------+----------------------+----------------------+--------------------+----------------------+----------------------+----------------------+-----------------------+\n",
      "|                   false|                 false|                 false|               false|                 false|                 false|                 false|                  false|\n",
      "|                   false|                 false|                 false|               false|                 false|                 false|                 false|                  false|\n",
      "|                   false|                 false|                 false|               false|                 false|                 false|                 false|                  false|\n",
      "|                   false|                 false|                 false|               false|                 false|                 false|                 false|                  false|\n",
      "|                   false|                 false|                 false|               false|                 false|                 false|                 false|                  false|\n",
      "|                   false|                 false|                 false|               false|                 false|                 false|                 false|                  false|\n",
      "|                   false|                 false|                 false|               false|                 false|                 false|                 false|                  false|\n",
      "|                   false|                 false|                 false|               false|                 false|                 false|                 false|                  false|\n",
      "|                   false|                 false|                 false|               false|                 false|                 false|                 false|                  false|\n",
      "|                   false|                 false|                 false|               false|                 false|                 false|                 false|                  false|\n",
      "|                   false|                 false|                 false|               false|                 false|                 false|                 false|                  false|\n",
      "|                   false|                 false|                 false|               false|                 false|                 false|                 false|                  false|\n",
      "|                   false|                 false|                 false|               false|                 false|                 false|                 false|                  false|\n",
      "|                   false|                 false|                 false|               false|                 false|                 false|                 false|                  false|\n",
      "|                   false|                 false|                 false|               false|                 false|                 false|                 false|                  false|\n",
      "|                   false|                 false|                 false|               false|                 false|                 false|                 false|                  false|\n",
      "|                   false|                 false|                 false|               false|                 false|                 false|                 false|                  false|\n",
      "|                   false|                 false|                 false|               false|                 false|                 false|                 false|                  false|\n",
      "|                   false|                 false|                 false|               false|                 false|                 false|                 false|                  false|\n",
      "|                   false|                 false|                 false|               false|                 false|                 false|                 false|                  false|\n",
      "+------------------------+----------------------+----------------------+--------------------+----------------------+----------------------+----------------------+-----------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Perform null value check\n",
    "null_values_check = fact_immigration.select(f.isnull('immigration_id'),\\\n",
    "                  f.isnull('arrival_time'),\\\n",
    "                  f.isnull('admission_id'),\\\n",
    "                  f.isnull('airport_id'),\\\n",
    "                  f.isnull('country_code'),\\\n",
    "                  f.isnull('arrival_mode'),\\\n",
    "                  f.isnull('arrival_year'),\\\n",
    "                  f.isnull('arrival_month'))\n",
    "\n",
    "null_values_check.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 4.3 Data dictionary \n",
    "Create a data dictionary for your data model. For each field, provide a brief description of what the data is and where it came from. You can include the data dictionary in the notebook or in a separate file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "- Data Dictionary for Immigrant dimension table:\n",
    "    \n",
    "| Column Name | Description | Origin |\n",
    "|-----|-----|-----|\n",
    "| admission_id | Unique identifier | i94 U.S. Immigration Dataset |\n",
    "| country_code | Country (list of valid and invalid 3-digit country codes) | i94 U.S. Immigration Dataset |\n",
    "| age | Age of Respondent in Years | i94 U.S. Immigration Dataset |\n",
    "| gender | Non-immigrant sex | i94 U.S. Immigration Dataset  |\n",
    "| visa_category | Visa code names collapsed into three categories: 1 = Business, 2 = Pleasure, 3 = Student | i94 U.S. Immigration Dataset |\n",
    "| visa_category_code | Visa codes collapsed into three categories: 1 = Business, 2 = Pleasure, 3 = Student | i94 U.S. Immigration Dataset |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "- Data Dictionary for Country dimension table:\n",
    "    \n",
    "| Column Name | Description | Origin |\n",
    "|-----|-----|-----|\n",
    "| country_code | Country (list of valid and invalid 3-digit country codes) | i94 U.S. Immigration Dataset |\n",
    "| country | Country name | i94 U.S. Immigration Dataset |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "- Data Dictionary for Time dimension table:\n",
    "    \n",
    "| Column Name | Description | Origin |\n",
    "|-----|-----|-----|\n",
    "| arrival_ts | Arrival date in the USA. Converted from SAS format. | i94 U.S. Immigration Dataset |\n",
    "| hour | Hour of arrival in the USA. | i94 U.S. Immigration Dataset |\n",
    "| day | Day of arrival in the USA. | i94 U.S. Immigration Dataset |\n",
    "| week | Week of arrival in the USA. | i94 U.S. Immigration Dataset |\n",
    "| month | Month of arrival in the USA. | i94 U.S. Immigration Dataset |\n",
    "| year | Year of arrival in the USA. | i94 U.S. Immigration Dataset |\n",
    "| weekday | Weekday of arrival in the USA. | i94 U.S. Immigration Dataset |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "- Data Dictionary for Airport dimension table:\n",
    "    \n",
    "| Column Name | Description | Origin |\n",
    "|-----|-----|-----|\n",
    "| airport_id | Unique identifier | World Aiport Code Dataset |\n",
    "| airport_type | Type of airport | World Aiport Code Dataset |\n",
    "| airport | Name of airport | World Aiport Code Dataset |\n",
    "| state_code | ISO region code of airport - state portion (2 letters) | World Aiport Code Dataset |\n",
    "| city | City of airport | World Aiport Code Dataset |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "- Data Dictionary for Immigration fact table:\n",
    "\n",
    "| Column Name | Description | Origin |\n",
    "|-----|-----|-----|\n",
    "| immigration_id | Unique identifier | i94 U.S. Immigration Dataset |\n",
    "| arrival_time | Arrival Date in the USA. It is a SAS date numeric field. | i94 U.S. Immigration Dataset |\n",
    "| admission_id | Admission Number | i94 U.S. Immigration Dataset |\n",
    "| airport_id | U.S. port of entry (list of valid and invalid 3-letter city/town codes) | i94 U.S. Immigration Dataset |\n",
    "| country_code | Country of residence (list of valid and invalid 3-digit country codes) | i94 U.S. Immigration Dataset |\n",
    "| arrival_mode | Mode of transport: 1 = 'Air', 2 = 'Sea', 3 = 'Land', 9 = 'Not reported' | i94 U.S. Immigration Dataset |\n",
    "| arrival_year | 4 digit year of entry | i94 U.S. Immigration Dataset |\n",
    "| arrival_month | Numeric month of entry | i94 U.S. Immigration Dataset |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Step 5: Complete Project Write Up\n",
    "* Clearly state the rationale for the choice of tools and technologies for the project.\n",
    "* Propose how often the data should be updated and why.\n",
    "* Write a description of how you would approach the problem differently under the following scenarios:\n",
    " * The data was increased by 100x.\n",
    " * The data populates a dashboard that must be updated on a daily basis by 7am every day.\n",
    " * The database needed to be accessed by 100+ people."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "1. **Rationale**\n",
    "\n",
    "   - I chose to use **Spark** because it is well-suited to processing large datasets, can be scaled up easily by simply adding nodes as the size of the datasets grow, and it can be used easily with cloud technologies such as **AWS S3** for storage and **AWS Redshift** for processing.\n",
    "\n",
    "   - I selected **Pandas** because it is familiar and easily used for smaller sets of data.\n",
    "\n",
    "   - I wrote the code in **Python** because this is one of the most widely used languages in Data Science and the most familiar to me. I also used a **Jupyter Notebook** because they lend themselves to *testing on the fly* and are useful for communicating multi-step processes to people less familiar with your program, or just programming in general."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "2. **Updates**\n",
    "\n",
    "   - Since the U.S. immigration data is provided in files grouped by month, it is probably available monthly. Therefore it should be updated at this same monthly frequency to ensure data is as fresh as possible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "3. **Scenarios**\n",
    "\n",
    "   - If **the data was increased by 100x**, then **Pandas** would need to be replaced by **Spark** as even the smallest datasets would get too unwield. In addition, I would switch storage from local to cloud storage like **AWS S3**, at least for staging the larger datasets. In that way, additional storage would simply require additional nodes, rather than a change in local hardware. To speed up analysis, it might also make sense to use **Amazon Redshift**.\n",
    "    \n",
    "   - If **the data populates a dashboard that must be updated on a daily basis by 7am every day**, then it would be better to switch from a mnual pipeline as developed here to an automated one, perhaps using **Apache Airflow**, which would support scheduling at the same time every day. The code could incorporate retries, and could send eamils on failure.\n",
    "   \n",
    "   - If **the database needed to be accessed by 100+ people**, then the output datasets (fact and dimension tables) should be stored in the cloud, rather than locally. They could use **AWS S3** for staging storage and **Amazon Redshift** for computations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
